Number of PMRs found:1
*********************************************************
PMR:        74542,057,649	Age: 5 

			Software PMR
Groups:     n/a
PPG:        203
Queue:      SUNGMC,165
Origin:     649 - Canada
Customer:   SO/BNS-DISTRIBUTED CustomerNumber: 0999569
Critsit:                  
Owner:      EL BARKOUKY, KARIM (028799)
Resolver:   EL BARKOUKY, KARIM (028799)
Title:      WFD - HA/XD site Failover hangs on lsfs on the DR node
Pri Sev:    2
Severity:   2
Priority:   2
System Down: 
Type:       Secondary
Category:   XSR
Component:  5765H3910    ( 712 )
APAR:       
Status:     OP1L1
English:    Y
Contact:    Muhammad Habib
	Phone1:   
	Phone2:   1-416-478-4097
	Email:    muhabib@ca.ibm.com
Created:    O17/08/11 22:38
Updated:    A17/08/16 08:35
Model:       Serial: 
Next Q:     HA,308
ccmsEntitled:IEEP
Follow-up Information:FUP/MM/DD
Keyword1:   KHBARK            
Keyword2:   NC       
Keyword3:                  
Scratchpad:                                                                         
Scratchpad:                                                                         
Scratchpad:                                                                         
Customer's Time:16 Aug 2017 20:10:20 GMT-05:00
Pages:      059
*********************************************************
* Entitlement                                           *
*********************************************************
IEEP00000999569 649INTERNAL EPA5045765H3910M5765H3
7 CANALL1 3 INTERNAL SUPPORT00000124
IEEPENTLNOHOURS 1200210012002100120021001200210012002100NOHOURS Entitlem
ent Ctr: IEE/EP EPA504 Item: 18121_81520.7944 Company: SO/BNS
-DISTRIBUTED Customer Number: 0999569 Contact: Habib, Muhamma
d Country: Canada TimeZone: EASTERN (WITH DST)
CPU/Serial: NONE OTHER Offering SRC: CTD Offering Id: CANAL
L1 Prime Service: INTERNAL SUPPORT - ALL SW PRODUC
Product ID: 5765H37 VRM: 712
Product Name: POWERHA SYSTEMMIRROR ENT CompID:5765H3910 Rel:713
CompID Name: POWERHA SYSTEMMIRROR XD 713 7X24 Ser
vice: This service provides offshift support for Sev1.
*********************************************************
* FORMATS                                               *
*********************************************************

*********************************************************
* NLS TEXT                                              *
*********************************************************
   +TY2K 007              -5765H3910  -L308/HA    -P2S2-17/08/11-21:38--CE
   S7> CALL SYS DOWN=                                                     
*** Electronic submission by customer via SR tool, version 3.4.7        

                                                              --NLSText Page:  2 --
*** Preferred contact method: Email-address.                            
*** Customer contact full name: Muhammad Habib                          
*** Telephone: 1-416-478-4097                                           
*** Email: muhabib@ca.ibm.com                                           
                                                                        
.                                                                       
Problem Details                                                         
.                                                                       
Product or Service: PowerHA SystemMirror 7.1 XD 7.1.3                   
Component ID: 5765H3910                                                 
.                                                                       
Operating System: AIX                                                   
.                                                                       
Problem title                                                           
PowerHA/XD site Failover hangs on lsfs on the DR node                   
.                                                                       

                                                              --NLSText Page:  3 --
Problem description and business impact                                 
Hi,                                                                     
We have two node cluster running PowerHA 7.1.3.5 .                      
Node "cs1cislq01" is primary node at Primary site "cs1" and node        
"cs2cislq01" is the DR node running at Secondary site "cs2".  HA starts 
and RG varies on without any issue on the primary node, however , when  
we do the failover to the DR node , while bringing up the filesystems   
,  it hangs while its running "lsfs" against the filesystem before it   
can bring up the filesystem.                                            
The disks are replicated using the Global mirror and GM is synced after 
the failover and no issue on the SAN side.  Further I have run the fsck 
against all filesystems and found not issues.  This only happens during 
the failover.                                                           
                                                                        
Please advise what logs you need.                                       
                                                                        

                                                              --NLSText Page:  4 --
.                                                                       
*** --- FOR SR USE ONLY ---                                             
*** XRQXSRprNode15561502504930588                                       
*** CAG2007464 ENG Y                                                    
*** MTS                                                                 
*** 5765H37:712/5765H3910:713 (ENT)                                     
   +TY2K 007              -5765H3910  -L12H/-------P2S2-17/08/11-21:38--AL
   S7> CALL SYS DOWN=                                                     
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S2-17/08/12-06:45--CT
   +EL BARKOUKY, KARIM    -5765H3910  -L55U/-------P2S2-17/08/12-06:45--AT
Contact Made by mail                                                    
   +EL BARKOUKY, KARIM    -5765H3910  -L55U/-------P2S2-17/08/12-06:45--AL
   S7> CALL SYS DOWN=                                                     
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/12-06:45--AT
ECuRep Mail Gateway - mail from support                                 
Send to: muhabib@ca.ibm.com                                             

                                                              --NLSText Page:  5 --
/ecurep/pmr/7/4/74542,057,649/mail20170812-134531-aix_support           
   +EL BARKOUKY, KARIM    -5765H3910  -L55U/-------P2S2-17/08/12-06:45--AL
   S7> CALL SYS DOWN=                                                     
   +EL BARKOUKY, KARIM    -5765H3910  -L55U/-------P2S2-17/08/12-06:45--AL
   S7> CALL SYS DOWN=                                                     
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S2-17/08/12-06:45--CR
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
   S7> CALL SYS DOWN=                                                     
sent a mail to the Cu requesting Data for analysis                      
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/12-08:51--AT
ECuRep Mail Gateway - Received Mail and stored in ECuRep                
Mail From: "Muhammad Habib" <muhabib@ca.ibm.com>                        
/ecurep/pmr/7/4/74542,057,649/mail20170812-155044-Muhammad_Habib        
File: 74542.057.649_lsfs_vg_info_DR_node.txt        5643 bytes          
File: mail.html                                   5441 bytes            

                                                              --NLSText Page:  6 --
File: mail.wri                                    2295 bytes            
   +ECUREP PMRUPDATE R7 P -5765H3910  -L308/HA    -P2S2-17/08/12-08:51-SCG
   +ECUREP PMRUPDATE R7 P -5765H3910  -L308/HA    -P2S2-17/08/12-08:51-SAT
   S7> CALL SYS DOWN=                                                     
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/12-08:55--AT
Material received from HTTP/SFTP server and stored in ECuRep:           
Directory: /ecurep/pmr/7/4/74542,057,649/2017-08-12                     
File: 74542.057.649.snap.pax.Z                           346132965 bytes
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/12-08:55--AT
No call generated. A Call exists already on Queue: HA,308               
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/12-08:56--AT
ECuRep AoA - directory: /ecurep/pmr/7/4/74542,057,649                   
unpacked 2017-08-12/74542.057.649.snap.pax.Z to directory               
/ecurep/pmr/7/4/74542,057,649/2017-08-12/74542.057.649.snap.pax.Z_unpack
find: There is no process to read data written to a pipe.               
ENVIRONMENT: 7100-04-03-1642                                            

                                                              --NLSText Page:  7 --
   Hostname: cs2cislq01                                                 
   Model:    IBM,8286-42A                                               
   Firmware: SV840_113 (t)                                              
   Serial:   IBM,0206E553R                                              
HACMP nodes and machine information:                                    
cs1cislq01:                                                             
        Machine/Cabinet Serial No...21B7CFV                             
        Machine Type and Model......8286-42A                            
cs2cislq01:                                                             
        Machine/Cabinet Serial No...06E553R                             
        Machine Type and Model......8286-42A                            
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/12-08:56--AT
No call generated. A Call exists already on Queue: HA,308               
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S2-17/08/12-09:21-SCR
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          

                                                              --NLSText Page:  8 --
   S7> CALL SYS DOWN=                                                     
will continu investigation and update pmr                               
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S2-17/08/14-07:35-SCR
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
   S7> CALL SYS DOWN=                                                     
will continu investigation                                              
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S2-17/08/14-08:29-SCT
   +EL BARKOUKY, KARIM    -5765H3910  -L308/-------P2S2-17/08/14-08:29--AT
Contact Made by mail                                                    
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S2-17/08/14-08:30-SCC
   S5> SERVICE GIVEN= 29  SG/29/                                          
   S6> SERVICE GIVEN= 29  SG/29/                                          
ENV:                                                                    
====                                                                    
                                                                        

                                                              --NLSText Page:  9 --
 General Cluster Information                                            
===========================                                             
* Cluster ID:   1458843297                                              
* Cluster Name: edmo-qat-cluster1                                       
* In Migration: no                                                      
* Node Name:    cs1cislq01                                              
* Cluster Type: linked                                                  
* CAA Heartbeat Type: unicast                                           
                                                                        
Topology                                                                
========                                                                
                                                                        
2 nodes:                                                                
--------                                                                
* cs1cislq01: id=1                                                      
  - AIX 7100-04-03-1642 (bos.mp64 7.1.4.31)                             

                                                              --NLSText Page:  10 --
  - CAA 7.1.4 (bos.cluster.rte 7.1.4.30 [no IFixes])                    
  - RSCT 3.2.1 (rsct.basic.rte 3.2.1.11 [no IFixes])                    
  - PowerHA 7.1.3 (cluster.es.server.rte 7.1.3.5 [no IFixes])           
* cs2cislq01: id=2                                                      
  - AIX 7100-04-03-1642 (bos.mp64 7.1.4.31)                             
  - CAA 7.1.4 (bos.cluster.rte 7.1.4.30 [no IFixes])                    
  - RSCT 3.2.1 (rsct.basic.rte 3.2.1.11 [no IFixes])                    
  - PowerHA 7.1.3 (cluster.es.server.rte 7.1.3.5 [no IFixes])           
                                                                        
2 sites:                                                                
--------                                                                
* cs1:                                                                  
  - id=1                                                                
  - nodes= cs1cislq01                                                   
  - dominance=yes                                                       
  - protection=NONE                                                     

                                                              --NLSText Page:  11 --
* cs2:                                                                  
  - id=2                                                                
  - nodes= cs2cislq01                                                   
  - dominance=no                                                        
  - protection=NONE                                                     
                                                                        
1 networks:                                                             
-----------                                                             
* net_XD_ip_01:                                                         
  - type=XD_ip                                                          
  - attr=public                                                         
  - IPAT: IPAT by aliasing enabled                                      
                                                                        
CAA tunables:                                                           
-------------                                                           
*                                                                       

                                                              --NLSText Page:  12 --
communication_mode,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a80
2):u                                                                    
*                                                                       
config_timeout,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802):2
40                                                                      
* deadman_mode,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802):a
  [Comment by this tool: [a: assert (node panic when DMS timeout        
expires)]]                                                              
*                                                                       
link_timeout,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802):300
00                                                                      
*                                                                       
local_merge_policy,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a80
2):m                                                                    
*                                                                       
network_fdt,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802):2000

                                                              --NLSText Page:  13 --
0                                                                       
*                                                                       
no_if_traffic_monitor,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361
a802):0                                                                 
  [Comment by this tool: [0: Interfaces will be declared down due to no 
traffic]]                                                               
*                                                                       
node_down_delay,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802):
10000                                                                   
  [Comment by this tool: [unit: millisec]]                              
*                                                                       
node_timeout,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802):300
00                                                                      
  [Comment by this tool: [unit: millisec]]                              
* packet_ttl,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802):32 
*                                                                       

                                                              --NLSText Page:  14 --
remote_hb_factor,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802)
:1                                                                      
* repos_mode,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802):e  
  [Comment by this tool: [e: event (no node panic when access to rep    
disk is lost)]]                                                         
*                                                                       
site_merge_policy,edmo-qat-cluster1(8ac2ee06-7efb-11e7-8002-9e1c4361a802
):h                                                                     
                                                                        
Resources                                                               
=========                                                               
                                                                        
1 groups:                                                               
---------                                                               
* edmo_rg1:                                                             
  - id=1                                                                

                                                              --NLSText Page:  15 --
  - nodes=cs1cislq01 cs2cislq01                                         
  - attributes: startup=OHN fallover=FNPN fallback=NFB                  
  - acquisition: parallel                                               
  - release: parallel                                                   
                                                                        
0 PARENT_CHILD RG Dependencies                                          
------------------------------                                          
                                                                        
0 START_AFTER RG Dependencies                                           
-----------------------------                                           
                                                                        
0 STOP_AFTER RG Dependencies                                            
----------------------------                                            
                                                                        
0 Location RG Dependencies                                              
--------------------------                                              

                                                              --NLSText Page:  16 --
                                                                        
Problem description:                                                    
====================                                                    
We have two node cluster running PowerHA 7.1.3.5 .                      
Node "cs1cislq01" is primary node at Primary site "cs1" and node        
"cs2cislq01" is the DR node running at Secondary site "cs2".  HA starts 
and RG varies on without any issue on the primary node, however , when  
we do the failover to the DR node , while bringing up the filesystems   
,  it hangs while its running "lsfs" against the filesystem before it   
can bring up the filesystem.                                            
The disks are replicated using the Global mirror and GM is synced after 
the failover and no issue on the SAN side.  Further I have run the fsck 
against all filesystems and found not issues.  This only happens during 
the failover.                                                           
                                                                        
TC                                                                      

                                                              --NLSText Page:  17 --
===                                                                     
snap -e                                                                 
                                                                        
logs:                                                                   
=====                                                                   
                                                                        
cs2cislq01                                                              
                                                                        
Aug 11 23:23:25 SYSJ2      I J2_FSCK_INFO        Filesystem             
/dev/infoserlv ENOEXEC                                                  
Aug 11 23:22:55 SYSJ2      I J2_FSCK_INFO        Filesystem             
/dev/infoserlv ENOEXEC                                                  
Aug 11 23:22:11 SYSJ2      I J2_FSCK_INFO        Filesystem             
/dev/infoserlv ENOEXEC                                                  
Aug 11 23:21:12 StorageRM  I STORAGERM_STARTED_S                        
Aug 11 23:21:09 ConfigRM   I CONFIGRM_ONLINE_ST                         

                                                              --NLSText Page:  18 --
Aug 11 23:21:09 ConfigRM   I CONFIGRM_HASQUORUM_                        
Aug 11 23:20:26 cthags     I GS_START_ST                                
Aug 11 23:19:18 ConfigRM   I CONFIGRM_STARTED_ST                        
Aug 11 23:19:16 RMCdaemon  I RMCD_INFO_0_ST                             
Aug 11 23:18:17 SYSPROC    T REBOOT_ID                                  
Aug 11 23:18:35 errdemon   T ERRLOG_ON                                  
Aug 11 22:59:29 OPERATOR   T OPMSG               clexit.rc : Normal     
termination of clstrmgrES. Restart now.                                 
Aug 11 22:59:29 SRC        P SRC_SVKO                                   
Aug 11 22:31:10 StorageRM  I STORAGERM_STARTED_S                        
Aug 11 22:31:10 ConfigRM   I CONFIGRM_ONLINE_ST                         
Aug 11 22:31:10 ConfigRM   I CONFIGRM_HASQUORUM_                        
Aug 11 22:30:23 cthags     I GS_START_ST                                
Aug 11 22:29:14 ConfigRM   I CONFIGRM_STARTED_ST                        
Aug 11 22:29:13 RMCdaemon  I RMCD_INFO_0_ST                             
Aug 11 22:28:15 SYSPROC    T REBOOT_ID                                  

                                                              --NLSText Page:  19 --
Aug 11 22:28:34 errdemon   T ERRLOG_ON                                  
Aug 11 21:59:08 OPERATOR   T OPMSG               clexit.rc : Normal     
termination of clstrmgrES. Restart now.                                 
Aug 11 21:59:08 SRC        P SRC_SVKO                                   
Aug 11 21:41:29 StorageRM  I STORAGERM_STARTED_S                        
Aug 11 21:41:28 ConfigRM   I CONFIGRM_ONLINE_ST                         
Aug 11 21:41:28 ConfigRM   I CONFIGRM_HASQUORUM_                        
Aug 11 21:40:42 cthags     I GS_START_ST                                
Aug 11 21:39:34 ConfigRM   I CONFIGRM_STARTED_ST                        
Aug 11 21:39:33 RMCdaemon  I RMCD_INFO_0_ST                             
Aug 11 21:38:31 SYSPROC    T REBOOT_ID                                  
Aug 11 21:38:51 errdemon   T ERRLOG_ON                                  
Aug 11 21:15:14 StorageRM  I STORAGERM_STARTED_S                        
Aug 11 21:15:14 ConfigRM   I CONFIGRM_ONLINE_ST                         
Aug 11 21:15:14 ConfigRM   I CONFIGRM_HASQUORUM_                        
Aug 11 21:15:14 RMCdaemon  I RMCD_INFO_3_ST                             

                                                              --NLSText Page:  20 --
Aug 11 21:15:13 ConfigRM   I CONFIGRM_STARTED_ST                        
Aug 11 21:15:13 SRC        I SRC_RSTRT                                  
Aug 11 21:15:13 ConfigRM   P CONFIGRM_EXIT_ONLIN                        
Aug 11 21:14:58 ConfigRM   P CONFIGRM_ONLINEFAIL                        
Aug 11 21:14:56 cthags     I GS_START_ST                                
Aug 11 21:05:13 ConfigRM   I CONFIGRM_STARTED_ST                        
Aug 11 21:05:12 RMCdaemon  I RMCD_INFO_0_ST                             
Aug 11 21:04:14 SYSPROC    T REBOOT_ID                                  
Aug 11 21:04:32 errdemon   T ERRLOG_ON                                  
Aug 11 21:02:54 errdemon   T ERRLOG_OFF                                 
Aug 11 21:01:46 ConfigRM   I CONFIGRM_OFFLINE_ST                        
Aug 11 21:01:45 cthags     I GS_STOP_ST                                 
Aug 11 21:01:45 RMCdaemon  I RMCD_INFO_4_ST                             
Aug 11 21:01:41 StorageRM  I STORAGERM_STOPPED_S                        
Aug 11 20:57:30 StorageRM  I STORAGERM_STARTED_S                        
                                                                        

                                                              --NLSText Page:  21 --
HACMP.out                                                               
==========                                                              
                                                                        
Aug 11 21:17:30 EVENT START: site_up cs2                                
Aug 11 21:17:30 EVENT START: site_up_local                              
Aug 11 21:17:30 EVENT COMPLETED: site_up_local 0                        
Aug 11 21:17:30 EVENT COMPLETED: site_up cs2 0                          
Aug 11 21:17:30 EVENT START: node_up cs2cislq01                         
Aug 11 21:17:31 EVENT COMPLETED: node_up cs2cislq01 0                   
Aug 11 21:17:33 EVENT START: node_up_complete cs2cislq01                
Aug 11 21:17:33 EVENT COMPLETED: node_up_complete cs2cislq01 0          
                                                                        
Aug 11 21:18:39 EVENT START: external_resource_state_change cs2cislq01  
<<<<                                                                    
Aug 11 21:18:39 EVENT COMPLETED: external_resource_state_change         
cs2cislq01 0                                                            

                                                              --NLSText Page:  22 --
Aug 11 21:18:39 EVENT START: rg_move_release cs1cislq01 1               
Aug 11 21:18:39 EVENT START: rg_move cs1cislq01 1 RELEASE               
Aug 11 21:18:39 EVENT COMPLETED: rg_move cs1cislq01 1 RELEASE 0         
Aug 11 21:18:39 EVENT COMPLETED: rg_move_release cs1cislq01 1 0         
Aug 11 21:18:46 EVENT START: rg_move_fence cs1cislq01 1                 
Aug 11 21:18:46 EVENT COMPLETED: rg_move_fence cs1cislq01 1 0           
Aug 11 21:18:48 EVENT START: rg_move_fence cs1cislq01 1  <<<<           
Aug 11 21:18:49 EVENT COMPLETED: rg_move_fence cs1cislq01 1 0           
Aug 11 21:18:49 EVENT START: rg_move_acquire cs1cislq01 1               
Aug 11 21:18:49 EVENT START: rg_move cs1cislq01 1 ACQUIRE               
Aug 11 21:18:49 EVENT START: acquire_takeover_addr                      
Aug 11 21:18:50 EVENT COMPLETED: acquire_takeover_addr 0                
Aug 11 21:24:48 EVENT START: config_too_long 360 TE_RG_MOVE_ACQUIRE <<<<
                                                                        
+edmo_rg1:cl_activate_fs[activate_fs_process_group:380] : Get unique    
temporary file names by using the resource group and the                

                                                              --NLSText Page:  23 --
+edmo_rg1:cl_activate_fs[activate_fs_process_group:381] : current       
process ID                                                              
+edmo_rg1:cl_activate_fs[activate_fs_process_group:383] [[ -z edmo_rg1  
]]                                                                      
+edmo_rg1:cl_activate_fs[activate_fs_process_group:392]                 
TMP_FILENAME=edmo_rg1_activate_fs.tmp24903736                           
+edmo_rg1:cl_activate_fs[activate_fs_process_group:393] rm -f           
/tmp/edmo_rg1_activate_fs.tmp24903736                                   
+edmo_rg1:cl_activate_fs[activate_fs_process_group:396] : If            
FSCHECK_TOOL is null get from ODM                                       
+edmo_rg1:cl_activate_fs[activate_fs_process_group:398] [[ -z fsck ]]   
+edmo_rg1:cl_activate_fs[activate_fs_process_group:403] print fsck      
+edmo_rg1:cl_activate_fs[activate_fs_process_group:403]                 
FSCHECK_TOOL=fsck                                                       
+edmo_rg1:cl_activate_fs[activate_fs_process_group:404] [[ fsck != fsck 
]]                                                                      

                                                              --NLSText Page:  24 --
+edmo_rg1:cl_activate_fs[activate_fs_process_group:411] : If            
RECOVERY_METHOD is null get from ODM                                    
+edmo_rg1:cl_activate_fs[activate_fs_process_group:413] [[ -z sequential
]]                                                                      
+edmo_rg1:cl_activate_fs[activate_fs_process_group:418] print sequential
+edmo_rg1:cl_activate_fs[activate_fs_process_group:418]                 
RECOVERY_METHOD=sequential                                              
+edmo_rg1:cl_activate_fs[activate_fs_process_group:419] [[ sequential !=
sequential ]]                                                           
+edmo_rg1:cl_activate_fs[activate_fs_process_group:426] set -u          
+edmo_rg1:cl_activate_fs[activate_fs_process_group:429] : If            
FSCHECK_TOOL is set to logredo, the logredo for each jfslog has         
+edmo_rg1:cl_activate_fs[activate_fs_process_group:430] : already been  
done in get_disk_vg_fs, so we only need to do fsck check                
+edmo_rg1:cl_activate_fs[activate_fs_process_group:431] : and recovery  
here before going on to do the mounts                                   

                                                              --NLSText Page:  25 --
+edmo_rg1:cl_activate_fs[activate_fs_process_group:433] [[ fsck == fsck 
]]                                                                      
+edmo_rg1:cl_activate_fs[activate_fs_process_group:436]                 
TOOL='/usr/sbin/fsck -f -p -o nologredo'                                
+edmo_rg1:cl_activate_fs:/autosys_old[activate_fs_process_group:440]    
PS4_LOOP=/autosys_old                                                   
+edmo_rg1:cl_activate_fs:/autosys_old[activate_fs_process_group:441]    
lsfs /autosys_old                                                       
+edmo_rg1:cl_activate_fs:/autosys_old[activate_fs_process_group:441]    
grep -w /autosys_old                                                    
                                                                        
Aug 11 21:24:48 EVENT START: config_too_long 360 TE_RG_MOVE_ACQUIRE     
                                                                        
                                                                        
...                                                                     
                                                                        

                                                              --NLSText Page:  26 --
Aug 11 23:08:26 EVENT START: external_resource_state_change cs2cislq01  
<<<                                                                     
Aug 11 23:08:26 EVENT COMPLETED: external_resource_state_change         
cs2cislq01 0                                                            
Aug 11 23:08:27 EVENT START: rg_move_release cs1cislq01 1               
Aug 11 23:08:27 EVENT START: rg_move cs1cislq01 1 RELEASE  <<<          
Aug 11 23:08:27 EVENT COMPLETED: rg_move cs1cislq01 1 RELEASE 0         
Aug 11 23:08:27 EVENT COMPLETED: rg_move_release cs1cislq01 1 0         
Aug 11 23:08:34 EVENT START: rg_move_fence cs1cislq01 1                 
Aug 11 23:08:34 EVENT COMPLETED: rg_move_fence cs1cislq01 1 0           
Aug 11 23:08:36 EVENT START: rg_move_fence cs1cislq01 1                 
Aug 11 23:08:36 EVENT COMPLETED: rg_move_fence cs1cislq01 1 0           
Aug 11 23:08:37 EVENT START: rg_move_acquire cs1cislq01 1               
Aug 11 23:08:37 EVENT START: rg_move cs1cislq01 1 ACQUIRE               
Aug 11 23:08:37 EVENT START: acquire_takeover_addr                      
Aug 11 23:08:38 EVENT COMPLETED: acquire_takeover_addr 0                

                                                              --NLSText Page:  27 --
Aug 11 23:14:36 EVENT START: config_too_long 360 TE_RG_MOVE_ACQUIRE <<<<
Aug 12 00:51:28 EVENT START: site_up cs2                                
Aug 12 00:51:29 EVENT START: site_up_local                              
Aug 12 00:51:29 EVENT COMPLETED: site_up_local 0                        
Aug 12 00:51:29 EVENT COMPLETED: site_up cs2 0                          
Aug 12 00:51:29 EVENT START: node_up cs2cislq01                         
Aug 12 00:51:29 EVENT COMPLETED: node_up cs2cislq01 0                   
Aug 12 00:51:32 EVENT START: node_up_complete cs2cislq01                
Aug 12 00:51:32 EVENT COMPLETED: node_up_complete cs2cislq01 0          
                                                                        
+edmo_rg1:cl_activate_fs[activate_fs_process_group:380] : Get unique    
temporary file names by using the resource group and the                
+edmo_rg1:cl_activate_fs[activate_fs_process_group:381] : current       
process ID                                                              
+edmo_rg1:cl_activate_fs[activate_fs_process_group:383] [[ -z edmo_rg1  
]]                                                                      

                                                              --NLSText Page:  28 --
+edmo_rg1:cl_activate_fs[activate_fs_process_group:392]                 
TMP_FILENAME=edmo_rg1_activate_fs.tmp19333368                           
+edmo_rg1:cl_activate_fs[activate_fs_process_group:393] rm -f           
/tmp/edmo_rg1_activate_fs.tmp19333368                                   
+edmo_rg1:cl_activate_fs[activate_fs_process_group:396] : If            
FSCHECK_TOOL is null get from ODM                                       
+edmo_rg1:cl_activate_fs[activate_fs_process_group:398] [[ -z fsck ]]   
+edmo_rg1:cl_activate_fs[activate_fs_process_group:403] print fsck      
+edmo_rg1:cl_activate_fs[activate_fs_process_group:403]                 
FSCHECK_TOOL=fsck                                                       
+edmo_rg1:cl_activate_fs[activate_fs_process_group:404] [[ fsck != fsck 
]]                                                                      
+edmo_rg1:cl_activate_fs[activate_fs_process_group:411] : If            
RECOVERY_METHOD is null get from ODM                                    
+edmo_rg1:cl_activate_fs[activate_fs_process_group:413] [[ -z sequential
]]                                                                      

                                                              --NLSText Page:  29 --
+edmo_rg1:cl_activate_fs[activate_fs_process_group:418] print sequential
+edmo_rg1:cl_activate_fs[activate_fs_process_group:418]                 
RECOVERY_METHOD=sequential                                              
+edmo_rg1:cl_activate_fs[activate_fs_process_group:419] [[ sequential !=
sequential ]]                                                           
+edmo_rg1:cl_activate_fs[activate_fs_process_group:426] set -u          
+edmo_rg1:cl_activate_fs[activate_fs_process_group:429] : If            
FSCHECK_TOOL is set to logredo, the logredo for each jfslog has         
+edmo_rg1:cl_activate_fs[activate_fs_process_group:430] : already been  
done in get_disk_vg_fs, so we only need to do fsck check                
+edmo_rg1:cl_activate_fs[activate_fs_process_group:431] : and recovery  
here before going on to do the mounts                                   
+edmo_rg1:cl_activate_fs[activate_fs_process_group:433] [[ fsck == fsck 
]]                                                                      
+edmo_rg1:cl_activate_fs[activate_fs_process_group:436]                 
TOOL='/usr/sbin/fsck -f -p -o nologredo'                                

                                                              --NLSText Page:  30 --
+edmo_rg1:cl_activate_fs:/datahub[activate_fs_process_group:440]        
PS4_LOOP=/datahub                                                       
+edmo_rg1:cl_activate_fs:/datahub[activate_fs_process_group:441] lsfs   
/datahub                                                                
+edmo_rg1:cl_activate_fs:/datahub[activate_fs_process_group:441] grep -w
/datahub                                                                
+edmo_rg1:cl_activate_fs:/datahub[activate_fs_process_group:441] read   
DEV rest                                                                
+edmo_rg1:cl_activate_fs:/datahub[activate_fs_process_group:442] [[     
sequential == parallel ]]                                               
+edmo_rg1:cl_activate_fs:/datahub[activate_fs_process_group:447]        
/usr/sbin/fsck -f -p -o nologredo /dev/datahublv                        
                                                                        
The current volume is: /dev/datahublv                                   
Primary superblock is valid.                                            
                                                                        

                                                              --NLSText Page:  31 --
                                                                        
+edmo_rg1:cl_activate_fs:/datahub/workq/Datasets1[activate_fs_process_gr
oup:440] PS4_LOOP=/datahub/workq/Datasets1                              
+edmo_rg1:cl_activate_fs:/datahub/workq/Datasets1[activate_fs_process_gr
oup:441] lsfs /datahub/workq/Datasets1                                  
+edmo_rg1:cl_activate_fs:/datahub/workq/Datasets1[activate_fs_process_gr
oup:441] grep -w /datahub/workq/Datasets1                               
                                                                        
Aug 11 23:14:36 EVENT START: config_too_long 360 TE_RG_MOVE_ACQUIRE     
                                                                        
                                                                        
Summary:                                                                
=========                                                               
                                                                        
the fail-over to the Dr site seem to be hanging, it doesn't complete in 
timely fashion, due to cl_activate_fs, while running lsfs on some FS, so

                                                              --NLSText Page:  32 --
powerHA log a config_too_long event as the event didn't complete in 360 
sec as configured.                                                      
                                                                        
we will need the following:                                             
- stop the cluster service on node cs2cislq01                           
- varyon the VGs and try to mount the applciation FS                    
then return the output of :                                             
#lsfs /autosys_old                                                      
#lsfs /datahub/workq/Datasets1                                          
                                                                        
also send me the /etc/filesystems file from both nodes please           
                                                                        
next action:                                                            
===========                                                             
informed the Cu by mail about this and waiting for Data                 
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/14-10:34--AT

                                                              --NLSText Page:  33 --
ECuRep Mail Gateway - mail from support                                 
Send to: "Muhammad Habib" <muhabib@ca.ibm.com>                          
/ecurep/pmr/7/4/74542,057,649/mail20170814-153001-aix_support           
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/14-10:38--AT
Material received from HTTP/SFTP server and stored in ECuRep:           
Directory: /ecurep/pmr/7/4/74542,057,649/2017-08-14                     
File: 74542.057.649.cs2cislq01.filesystems                    4125 bytes
File: 74542.057.649.cs1cislq01.filesystems                    4235 bytes
   +ECUREP PMRUPDATE R7 P -5765H3910  -L308/HA    -P2S2-17/08/14-10:38-SCG
   +ECUREP PMRUPDATE R7 P -5765H3910  -L308/HA    -P2S2-17/08/14-10:38-SAT
   S7> CALL SYS DOWN=                                                     
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/14-10:43--AT
ECuRep Mail Gateway - Received Mail and stored in ECuRep                
Mail From: "Muhammad Habib" <muhabib@ca.ibm.com>                        
/ecurep/pmr/7/4/74542,057,649/mail20170814-155059-Muhammad_Habib        
File: mail.html                                  21148 bytes            

                                                              --NLSText Page:  34 --
File: mail.wri                                   17220 bytes            
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/14-10:43--AT
No call generated. A Call exists already on Queue: HA,308               
   +EL BARKOUKY, KARIM    -5765H3910  -L308/-------P2S2-17/08/14-11:11--AL
   S7> CALL SYS DOWN=                                                     
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S2-17/08/14-11:11-SCR
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S2-17/08/14-11:11-SAT
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
   S7> CALL SYS DOWN=                                                     
   +EL BARKOUKY, KARIM    -5765H3910  -L308/-------P2S2-17/08/15-04:54--AL
   S7> CALL SYS DOWN=                                                     
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S1-17/08/15-04:54-SCR
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
   S7> CALL SYS DOWN=                                                     

                                                              --NLSText Page:  35 --
the CU sent the following                                               
                                                                        
=====================Mail-st==================                          
Hi Karim,                                                               
                                                                        
I have uploaded /etc/filesystems from both servers.                     
                                                                        
Further I tried to run the lsfs manually on these filesystems while     
cluster was in failover mode to DR over the weekend. I also checked the 
GM was synced (and GM direction was from DR to Primary site) , However, 
lsfs was hanging when running lsfs on the following filesytems: ( in RED
and bold). Other filesystems were fine.                                 
                                                                        
cs2cislq01:root:/home/root#lsvg -l datavg                               
datavg:                                                                 
LV NAME TYPE LPs PPs PVs LV STATE MOUNT POINT                           

                                                              --NLSText Page:  36 --
loglv01 jfs2log 1 1 1 closed/syncd N/A                                  
db2lv jfs2 21 21 1 closed/syncd /opt/IBM/db2_old                        
datahublv jfs2 2653 2653 3 closed/syncd /datahub                        
infoserlv jfs2 542 542 2 closed/syncd /opt/IBM/InformationServer        
<<<===== bad                                                            
datasets1lv jfs2 240 240 2 open/syncd /datahub/workq/Datasets1 <<< =====
bad                                                                     
datasets2lv jfs2 240 240 2 closed/syncd /datahub/workq/Datasets2        
<<<=====bad                                                             
scratch1lv jfs2 240 240 2 closed/syncd /datahub/workq/Scratch1          
scratch2lv jfs2 240 240 2 closed/syncd /datahub/workq/Scratch2          
dbsqlblv jfs2 20 20 1 closed/syncd /sqllib_inst1                        
                                                                        
                                                                        
I would need to take downtime from client to bring up datavg at DR site 
again as GM will needs to be reversed manually if its to be done outside

                                                              --NLSText Page:  37 --
the HA. Please let me know if any other data to be collected when       
running lsfs on these filesystems once I get the window from client.    
                                                                        
Thanks                                                                  
=====================Mail-End=================                          
                                                                        
                                                                        
next action:                                                            
============                                                            
since the cmd lsfs is hanging againest 3 FS, I will send a sec to LVM   
team to check this deeper from lvm side                                 
   +MYQM GLOBAL_P_BOS  ROB-5765H3910  -B724/-------P2S2-17/08/15-04:55--AL
   S7> CALL SYS DOWN=                                                     
   +MYQM GLOBAL_P_BOS  ROB-5765H3910  -B724/-------P2S2-17/08/15-04:55--AL
   S7> CALL SYS DOWN=                                                     
   +MYQM GLOBAL_P_BOS  ROB-5765H3910  -B724/-------P2S2-17/08/15-04:55--AT

                                                              --NLSText Page:  38 --
<QM> Assigned to Rana H. Harfoush                                       
   +HARFOUSH, RANA        -5765H3910  -L308/-------P2S2-17/08/15-05:01--AL
   S7> CALL SYS DOWN=                                                     
   +HARFOUSH, RANA        -5765H3910  -L308/KHBARK-P2S2-17/08/15-05:01--CR
   +HARFOUSH, RANA        -5765H3910  -L308/KHBARK-P2S2-17/08/15-05:01--AT
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
   S7> CALL SYS DOWN=                                                     
   +MYQM GLOBAL_P_BOS  ROB-5765H3910  -B724/-------P2S2-17/08/15-05:07--AL
   S7> CALL SYS DOWN=                                                     
   +MYQM GLOBAL_P_BOS  ROB-5765H3910  -B724/-------P2S2-17/08/15-05:07--AT
<QM> Assigned to Riham I El-khatib                                      
   +EL-KHATIB, RIHAM      -5765H3910  -L519/-------P2S2-17/08/15-05:53--AT
.                                                                       
ACTION TAKEN:                                                           
=============                                                           

                                                              --NLSText Page:  39 --
                                                                        
It is not clear if the filesystem is corrupt or not,                    
                                                                        
This might be one of 2 scenarios:                                       
                                                                        
1. The FSs is corrupted                                                 
In this case we need to collect FS metadata ffrom the DR node and send  
to WWFSLV,165 for analysis                                              
                                                                        
2. It is simply a command hang (The lsfs command itself hangs)          
In that case we need to collect a pdump of the command at time of hang  
and analyse it to see where it gets stuck                               
                                                                        
.                                                                       
ACTION PLAN:                                                            
============                                                            

                                                              --NLSText Page:  40 --
ask for metadata and pdump for further analysis                         
   +EL-KHATIB, RIHAM      -5765H3910  -L308/LVM   -P2S1-17/08/15-06:14-SCT
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/15-06:14--AT
ECuRep Mail Gateway - mail from support                                 
------------------------------------------------------------------------
Send To: "Muhammad Habib" <muhabib@ca.ibm.com>                          
/ecurep/pmr/7/4/74542,057,649/mail20170815-131422-aix_support           
File: mail.html                                  27341 bytes            
File: mail.wri                                   17220 bytes            
---------------------------- EMAIL TEXT START --------------------------
Hello Muhammad,                                                         
                                                                        
This is Riham from the AIX LVM Team, and i will be assisting you on your
problem from the LVM side.                                              
                                                                        
Kindly upload the following data for further analysis.                  

                                                              --NLSText Page:  41 --
The data is to be collected from the DR node, while the cluster services
are down and the VG is varried on.                                      
                                                                        
- pdump for the hanging lsfs command (while hanging). This is how it    
goes;                                                                   
1- Please download the pdump.sh tool from the below link:               
ftp://ftp.software.ibm.com/aix/tools/debug                              
                                                                        
2- Upload it into your AIX system (If FTP, use binary mode),            
And give it an execution permission.                                    
                                                                        
3- Kill the hanging process.                                            
                                                                        
4- Open the shell, and recreate the issue. (In you case run fsls against
one of the hanging Filesystems. And wait till the command hangs)        
                                                                        

                                                              --NLSText Page:  42 --
5- Open a new (other) shell, and run pdump.sh against the hanging       
process:                                                                
# ./pdump.sh -l <pid>                                                   
(the pid can be acquired using "ps -ef" command)                        
                                                                        
6- After a while, you'll find in your path a new file created,          
Example: pdump.process_name.9044192.27Apr2015-08.22.00.out              
- Please upload it to ecurep.                                           
                                                                        
** Repeat the same for all the Filesystems that are hanging**           
                                                                        
- Please collect data for JFS2 Filesystem corruption for the 3          
Filesystems in question. Please use the below link on how to collect    
these data                                                              
http://www-01.ibm.com/support/docview.wss?uid=isg3T1011157              
                                                                        

                                                              --NLSText Page:  43 --
I will be waiting for the data when you take the downtime.              
                                                                        
Thank you for contacting IBM support.                                   
Riham El-Khatib                                                         
AIX Support                                                             
---------------------------- EMAIL TEXT END ----------------------------
   +EL-KHATIB, RIHAM      -5765H3910  -L308/LVM   -P2S1-17/08/15-06:15-SCR
   +EL-KHATIB, RIHAM      -5765H3910  -L308/RIHAM -P2S1-17/08/15-06:15-SAT
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
   S7> CALL SYS DOWN=                                                     
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/15-18:34--AT
ECuRep Mail Gateway - Received Mail and stored in ECuRep                
Mail From: "Muhammad Habib" <muhabib@ca.ibm.com>                        
/ecurep/pmr/7/4/74542,057,649/mail20170816-013343-Muhammad_Habib        
File: mail.html                                  25149 bytes            

                                                              --NLSText Page:  44 --
File: mail.wri                                   20192 bytes            
   +ECUREP PMRUPDATE R7 P -5765H3910  -L308/LVM   -P2S2-17/08/15-18:34-SCG
   +ECUREP PMRUPDATE R7 P -5765H3910  -L308/LVM   -P2S2-17/08/15-18:34-SAT
   S7> CALL SYS DOWN=                                                     
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/15-18:34--AT
integrity test on file                                                  
2017-08-16/74542.057.649.pdumpAndfilesystem.out.tar : ok                
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/15-18:34--AT
Material received from HTTP/SFTP server and stored in ECuRep:           
Directory: /ecurep/pmr/7/4/74542,057,649/2017-08-16                     
File: 74542.057.649.pdumpAndfilesystem.out.tar              931840 bytes
File: 74542.057.649.myscript.out                              2806 bytes
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/15-18:34--AT
No call generated. A Call exists already on Queue: LVM,308              
   +EL-KHATIB, RIHAM      -5765H3910  -L519/-------P2S2-17/08/16-07:51--AL
   S7> CALL SYS DOWN=                                                     

                                                              --NLSText Page:  45 --
   +EL-KHATIB, RIHAM      -5765H3910  -L308/LVM   -P2S2-17/08/16-07:51-SCR
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
   S7> CALL SYS DOWN=                                                     
.                                                                       
CUSTOMER REP: Muhammad Habib                                            
=============                                                           
                                                                        
ACTION TAKEN:                                                           
=============                                                           
* From pdump:                                                           
-------------                                                           
# proctree 3801120                                                      
2555936    /usr/sbin/srcmstr                                            
   6946856    /usr/sbin/sshd                                            
      12058876    sshd: fcid6 [priv]                                    

                                                              --NLSText Page:  46 --
         11993222    sshd: fcid6@pts/0                                  
            11206832    -ksh                                            
               15990812    sh -- /usr/bin/sucommand                     
                  12648526    -ksh                                      
                     3801120    lsfs /opt/IBM/InformationServer         
                        8650938    /sbin/helpers/jfs2/statfs64 -o 4     
/dev/infoserlv                                                          
                                                                        
# proctree 3801134                                                      
2555936    /usr/sbin/srcmstr                                            
   6946856    /usr/sbin/sshd                                            
      12058876    sshd: fcid6 [priv]                                    
         11993222    sshd: fcid6@pts/0                                  
            11206832    -ksh                                            
               15990812    sh -- /usr/bin/sucommand                     
                  12648526    -ksh                                      

                                                              --NLSText Page:  47 --
                     3801134    lsfs /datahub/workq/Datasets1           
                        8650946    /sbin/helpers/jfs2/statfs64 -o 4     
/dev/datasets1lv                                                        
                                                                        
# proctree 8650948                                                      
2555936    /usr/sbin/srcmstr                                            
   6946856    /usr/sbin/sshd                                            
      12058876    sshd: fcid6 [priv]                                    
         11993222    sshd: fcid6@pts/0                                  
            11206832    -ksh                                            
               15990812    sh -- /usr/bin/sucommand                     
                  12648526    -ksh                                      
                     8650948    lsfs /datahub/workq/Datasets2           
                        3801136    /sbin/helpers/jfs2/statfs64 -o 4     
/dev/datasets2lv                                                        
                                                                        

                                                              --NLSText Page:  48 --
First pdump                                                             
------------                                                            
pvthread+027F00 STACK:                                                  
[0010A830]e_block_thread+000290 ()                                      
[0010B3A8]e_sleep_thread+0000E8 (??, ??, ??)                            
[00802650]fifo_read+0000B0 (??, ??, ??, ??, ??)                         
[00802964]fifo_erdwr+000064 (??, ??, ??, ??, ??, ??, ??, ??)            
[00801DE4]fifo_rdwr+000024 (??, ??, ??, ??, ??, ??, ??, ??)             
[00690BF8]vnop_rdwr+0001B8 (??, ??, ??, ??, ??, ??, ??, ??)             
[0069D710]vno_rw+000150 (??, ??, ??, ??, ??)                            
[005FAFC8]rwuio+000108 (??, ??, ??, ??, ??, ??)                         
[005FB2E8]rdwr+000188 (??, ??, ??, ??, ??, ??)                          
[005FAE80]keread+000140 (??, ??, ??, ??)                                
[00003938]syscall+000230 ()                                             
[D0122DC8]read+000288 (??, ??, ??)                                      
[10000B4C]call_newhelper+0000CC (2FF22150, 000000B0, 2FF22284, 20000D28,

                                                              --NLSText Page:  49 --
   200153B8, 00000000, 505E5C00, 00000000)                              
[100007FC]getsize+0000BC (??, ??, ??, ??)                               
[100065E8]prnt_stanza+000148 (??, ??, ??)                               
[10006E18]do_ls+0001F8 (??, ??)                                         
[100035E8]main+000108 (??, ??)                                          
[10000168]__start+000068 ()                                             
[kdb_read_mem] no real storage @ FFFFFFFFFFF92A0                        
                                                                        
Second pdump                                                            
------------                                                            
pvthread+02DA00 STACK:                                                  
[0010A830]e_block_thread+000290 ()                                      
[0010B3A8]e_sleep_thread+0000E8 (??, ??, ??)                            
[00802650]fifo_read+0000B0 (??, ??, ??, ??, ??)                         
[00802964]fifo_erdwr+000064 (??, ??, ??, ??, ??, ??, ??, ??)            
[00801DE4]fifo_rdwr+000024 (??, ??, ??, ??, ??, ??, ??, ??)             

                                                              --NLSText Page:  50 --
[00690BF8]vnop_rdwr+0001B8 (??, ??, ??, ??, ??, ??, ??, ??)             
[0069D710]vno_rw+000150 (??, ??, ??, ??, ??)                            
[005FAFC8]rwuio+000108 (??, ??, ??, ??, ??, ??)                         
[005FB2E8]rdwr+000188 (??, ??, ??, ??, ??, ??)                          
[005FAE80]keread+000140 (??, ??, ??, ??)                                
[00003938]syscall+000230 ()                                             
[D0122DC8]read+000288 (??, ??, ??)                                      
[10000B4C]call_newhelper+0000CC (2FF22150, 000000B0, 2FF22284, 20000D28,
   200151F8, 00000000, 505EB000, 00000000)                              
[100007FC]getsize+0000BC (??, ??, ??, ??)                               
[100065E8]prnt_stanza+000148 (??, ??, ??)                               
[10006E18]do_ls+0001F8 (??, ??)                                         
[100035E8]main+000108 (??, ??)                                          
[10000168]__start+000068 ()                                             
[kdb_read_mem] no real storage @ FFFFFFFFFFF92A0                        
                                                                        

                                                              --NLSText Page:  51 --
3rd pdump                                                               
----------                                                              
pvthread+02D400 STACK:                                                  
[0010A830]e_block_thread+000290 ()                                      
[0010B3A8]e_sleep_thread+0000E8 (??, ??, ??)                            
[00802650]fifo_read+0000B0 (??, ??, ??, ??, ??)                         
[00802964]fifo_erdwr+000064 (??, ??, ??, ??, ??, ??, ??, ??)            
[00801DE4]fifo_rdwr+000024 (??, ??, ??, ??, ??, ??, ??, ??)             
[00690BF8]vnop_rdwr+0001B8 (??, ??, ??, ??, ??, ??, ??, ??)             
[0069D710]vno_rw+000150 (??, ??, ??, ??, ??)                            
[005FAFC8]rwuio+000108 (??, ??, ??, ??, ??, ??)                         
[005FB2E8]rdwr+000188 (??, ??, ??, ??, ??, ??)                          
[005FAE80]keread+000140 (??, ??, ??, ??)                                
[00003938]syscall+000230 ()                                             
[D0122DC8]read+000288 (??, ??, ??)                                      
[10000B4C]call_newhelper+0000CC (2FF22150, 000000B0, 2FF22284, 20000D28,

                                                              --NLSText Page:  52 --
   20015038, 00000000, 505E3400, 00000000)                              
[100007FC]getsize+0000BC (??, ??, ??, ??)                               
[100065E8]prnt_stanza+000148 (??, ??, ??)                               
[10006E18]do_ls+0001F8 (??, ??)                                         
[100035E8]main+000108 (??, ??)                                          
[10000168]__start+000068 ()                                             
[kdb_read_mem] no real storage @ FFFFFFFFFFF92A0                        
                                                                        
                                                                        
* myscript file, shows corruption on FS                                 
----------------------------------------                                
                                                                        
The current volume is: /dev/infoserlv                                   
Primary superblock is valid.                                            
Invalid data detected in aggregate inode 1.                             
fsck: 0507-278 Cannot continue.                                         

                                                              --NLSText Page:  53 --
File system is dirty.                                                   
File system is dirty but is marked clean.  In its present state,        
         the results of accessing /dev/infoserlv (except by             
         this command) are undefined.                                   
cs2cislq01:root:/tmp/pdump#umount /newfs                                
cs2cislq01:root:/tmp/pdump#backup -f                                    
metacaputre.opt.IBM.InformationServer.out -0 /newfs                     
backup: Date of this level 0 backup: Tue Aug 15 19:17:31 EDT 2017       
backup: Date of last level 0 backup: the epoch                          
backup: Backing up /dev/rfslv03 (/newfs) to                             
metacaputre.opt.IBM.InformationServer.out                               
backup: Mapping (Pass I) [regular files]                                
backup: Mapping (Pass II) [directories]                                 
backup: estimated 43 1k blocks.                                         
backup: Backing up (Pass III) [directories]                             
backup: Backing up (Pass IV) [regular files]                            

                                                              --NLSText Page:  54 --
backup: 60 1k blocks on 1 volume(s)                                     
backup: Backup is complete                                              
cs2cislq01:root:/tmp/pdump#fsck -yvv  /opt/IBM/InformationServer        
                                                                        
                                                                        
                                                                        
The current volume is: /dev/infoserlv                                   
Primary superblock is valid.                                            
J2_LOGREDO:log redo processing for /dev/infoserlv                       
j2_logredo.cpp: ERROR in initLogFSLocation line 2305, sb_open           
j2_logredo.cpp: ERROR in j2_logredo line 669, open/read device          
Failure replaying log: -1                                               
J2_LOGREDO:log redo processing for /dev/infoserlv                       
j2_logredo.cpp: ERROR in initLogFSLocation line 2305, sb_open           
j2_logredo.cpp: ERROR in j2_logredo line 669, open/read device          
Failure replaying log: -1                                               

                                                              --NLSText Page:  55 --
logredo failed (rc=255).  fsck continuing.                              
Primary superblock is valid.                                            
        Superblock s_state = 0x0 mode = 0x3                             
Invalid data detected in aggregate inode 1.                             
fsck: 0507-278 Cannot continue.                                         
File system is dirty.                                                   
Superblock is not marked dirty (FIXED)                                  
cs2cislq01:root:/tmp/pdump#lsfs /opt/IBM/InformationServer              
Name            Nodename   Mount Pt               VFS   Size    Options 
Auto Accounting                                                         
   cs2cislq01:root:/tmp/pdump#                                            
cs2cislq01:root:/tmp/pdump#                                             
cs2cislq01:root:/tmp/pdump#                                             
cs2cislq01:root:/tmp/pdump#                                             
cs2cislq01:root:/tmp/pdump#/sbin/helpers/jfs2/fscklog                   
/opt/IBM/InformationServer > fscklog.opt.IBM.InformationServer.out      

                                                              --NLSText Page:  56 --
cs2cislq01:root:/tmp/pdump#/sbin/helpers/jfs2/fscklog                   
/datahub/workq/Datasets1 >  fscklog.datahub.workq.Datasets1.out         
cs2cislq01:root:/tmp/pdump#/sbin/helpers/jfs2/fscklog                   
/datahub/workq/Datasets2 >  fscklog.datahub.workq.Datasets2.out         
cs2cislq01:root:/tmp/pdump#exit                                         
                                                                        
                                                                        
ACTION PLAN:                                                            
============                                                            
Sending secondary to WWFSLV,165 to check metadata and advise further    
LPWO to followup as needed                                              
                                                                        
TESTCASE:                                                               
=========                                                               
74542.057.649.pdumpAndfilesystem.out.tar                                
74542.057.649.myscript.out                                              

                                                              --NLSText Page:  57 --
   +EL-KHATIB, RIHAM      -5765H3910  -L519/-------P2S2-17/08/16-07:51--AL
   S7> CALL SYS DOWN=                                                     
   +EL-KHATIB, RIHAM      -5765H3910  -L519/-------P2S2-17/08/16-07:51--AL
   S7> CALL SYS DOWN=                                                     
   +EL-KHATIB, RIHAM      -5765H3910  -L308/RIHAM -P2S1-17/08/16-08:02-SCR
   +EL-KHATIB, RIHAM      -5765H3910  -L308/HA    -P2S1-17/08/16-08:02-SAT
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
   S7> CALL SYS DOWN=                                                     
   +CHUN, SUNG            -5765H3910  -L165/WWFSLV-P2S2-17/08/16-08:11-SCT
   +CHUN, SUNG            -5765H3910  -L165/-------P2S2-17/08/16-08:11--AT
Contact Made                                                            
   +CHUN, SUNG            -5765H3910  -L165/WWFSLV-P2S2-17/08/16-08:11-SCR
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
   S7> CALL SYS DOWN=                                                     

                                                              --NLSText Page:  58 --
ACTION TAKEN: Sent email to customer.                                   
Sung is new owner from WWFSLV                                           
ACTION PLAN: owner to study PMR and follow up                           
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/16-08:12--AT
ECuRep Mail Gateway - mail from support                                 
Send to: muhabib@ca.ibm.com                                             
/ecurep/pmr/7/4/74542,057,649/mail20170816-151123-Sung_M_Chun           
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/16-08:17--AT
ECuRep Mail Gateway - Received Mail and stored in ECuRep                
Mail From: "Muhammad Habib" <muhabib@ca.ibm.com>                        
/ecurep/pmr/7/4/74542,057,649/mail20170816-151706-Muhammad_Habib        
File: mail.html                                   3447 bytes            
File: mail.wri                                    1247 bytes            
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/16-08:17--AT
No call generated. A Call exists already on Queue: HA,308               
   +ECUREP PMRUPDATE R7 P -5765H3910  -L503/-------P2S2-17/08/16-08:21--AT

                                                              --NLSText Page:  59 --
ECuRep Mail Gateway - mail from support                                 
Send to: "Muhammad Habib" <muhabib@ca.ibm.com>                          
/ecurep/pmr/7/4/74542,057,649/mail20170816-152047-Sung_M_Chun           
   +EL BARKOUKY, KARIM    -5765H3910  -L308/HA    -P2S1-17/08/16-08:35-SCC
   S5> SERVICE GIVEN= 99  SG/99/                                          
   S6> SERVICE GIVEN= 99  SG/99/                                          
saw L2 update, closing sec                                              
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
***************************************************************************
